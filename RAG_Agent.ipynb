{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b362d5d",
   "metadata": {},
   "source": [
    "# Env Variables & Params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32eb3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils.config import CONFIG\n",
    "\n",
    "load_dotenv(dotenv_path=\".env.local\")\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.getenv('LANGCHAIN_TRACING_V2')\n",
    "LANGCHAIN_ENDPOINT = os.getenv('LANGCHAIN_ENDPOINT')\n",
    "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "LANGCHAIN_PROJECT = os.getenv('LANGCHAIN_PROJECT')\n",
    "\n",
    "FIRECRAWL_API_KEY = os.getenv('FIRECRAWL_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "\n",
    "CHUNK_SIZE = CONFIG[\"CHUNK_SIZE\"]\n",
    "CHUNK_OVERLAP = CONFIG[\"CHUNK_OVERLAP\"]\n",
    "LOCAL_LLM = CONFIG[\"LOCAL_LLM\"]\n",
    "URLS = CONFIG[\"URLS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d972c0",
   "metadata": {},
   "source": [
    "# Retrieve Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80031b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ…] Scraped: https://python.langchain.com/docs/introduction/\n",
      "[â„¹ï¸] Content length: 10885\n",
      "[âœ…] Scraped: https://www.ai-jason.com/learning-ai/how-to-reduce-llm-cost\n",
      "[â„¹ï¸] Content length: 7064\n",
      "[âœ…] Scraped: https://www.ai-jason.com/learning-ai/ai-agent-vision-tutorial\n",
      "[â„¹ï¸] Content length: 11777\n",
      "[âœ…] Scraped: https://www.ai-jason.com/learning-ai/ai-research-agent\n",
      "[â„¹ï¸] Content length: 5937\n",
      "[âœ…] Scraped: https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/\n",
      "[â„¹ï¸] Content length: 13164\n",
      "[âœ…] Scraped: https://arxiv.org/abs/2110.04599\n",
      "[â„¹ï¸] Content length: 7248\n",
      "[â„¹ï¸] Docs already flat, count: 6\n",
      "[â„¹ï¸] Split into 80 document chunks\n",
      "[â„¹ï¸] Filtered docs count: 80\n",
      "[âœ…] Vectorstore created and retriever initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders.firecrawl import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "import requests\n",
    "\n",
    "def scrape_with_firecrawl(url):\n",
    "    endpoint = \"https://api.firecrawl.dev/v1/scrape\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {FIRECRAWL_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    body = {\n",
    "        \"url\": url,\n",
    "        # \"mode\": \"scrape\", # deprecated\n",
    "        \"formats\": [\"markdown\"], # Request markdown directly\n",
    "        \"onlyMainContent\": True,\n",
    "        \"removeBase64Images\": True,\n",
    "        \"blockAds\": True,\n",
    "        \"proxy\": \"basic\",\n",
    "        \"timeout\": 30000\n",
    "    }\n",
    "    response = requests.post(endpoint, headers=headers, json=body)\n",
    "\n",
    "    if response.status_code == 403:\n",
    "        print(f\"[âŒ BLOCKED] {url} - This site is not supported by Firecrawl.\")\n",
    "        return None\n",
    "    if response.status_code != 200:\n",
    "        print(f\"[âŒ FAIL] {url} - Status: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "    \n",
    "    result = response.json()\n",
    "    content = result.get(\"data\", {}).get(\"markdown\", \"\").strip()\n",
    "\n",
    "    if not content:\n",
    "        print(f\"[âš ï¸] No content extracted from {url}\")\n",
    "        return None\n",
    "\n",
    "    return Document(page_content=content, metadata={\"url\": url})\n",
    "\n",
    "docs = []\n",
    "for url in URLS:\n",
    "    try:\n",
    "        doc = scrape_with_firecrawl(url)\n",
    "        if doc and doc.page_content:\n",
    "            docs.append(doc)\n",
    "            print(f\"[âœ…] Scraped: {url}\")\n",
    "        print(f\"[â„¹ï¸] Content length: {len(doc.page_content) if doc else 0}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[âŒ] Failed: {url} - {e}\")\n",
    "\n",
    "# split docs\n",
    "try:\n",
    "    if docs and isinstance(docs[0], list):\n",
    "        docs_list = [item for sublist in docs for item in sublist]\n",
    "        print(f\"[â„¹ï¸] Flattened docs count: {len(docs_list)}\")\n",
    "    else:\n",
    "        docs_list = docs\n",
    "        print(f\"[â„¹ï¸] Docs already flat, count: {len(docs_list)}\")\n",
    "except Exception as e:\n",
    "    print(f\"[âŒ] Error flattening docs list: {e}\")\n",
    "    docs_list = docs  # fallback\n",
    "\n",
    "try:\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=CHUNK_SIZE, \n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    docs_split = text_splitter.split_documents(docs_list)\n",
    "    print(f\"[â„¹ï¸] Split into {len(docs_split)} document chunks\")\n",
    "except Exception as e:\n",
    "    print(f\"[âŒ] Error during document splitting: {e}\")\n",
    "\n",
    "# filter out complex metadata, ensure proper doc format\n",
    "filtered_docs = []\n",
    "for i, doc in enumerate(docs_split):\n",
    "    try:\n",
    "        # ensure doc is an instance of Document & has a 'metadata' attribute\n",
    "        if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "            clean_metadata = {k: v for k, v in doc.metadata.items() if not isinstance(v, (str, int, float, bool))}\n",
    "            filtered_doc = Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata=clean_metadata\n",
    "            )\n",
    "            filtered_docs.append(filtered_doc)\n",
    "        else:\n",
    "            print(f\"[âš ï¸] Skipping doc at index {i}: Invalid type or missing metadata\")\n",
    "    except Exception as e:\n",
    "        print(f\"[âŒ] Error processing doc at index {i}: {e}\")\n",
    "\n",
    "print(f\"[â„¹ï¸] Filtered docs count: {len(filtered_docs)}\")\n",
    "\n",
    "# add to vectorDb\n",
    "try:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=filtered_docs,\n",
    "        collection_name=\"rag-chroma\",\n",
    "        embedding=GPT4AllEmbeddings(),\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    print(f\"[âœ…] Vectorstore created and retriever initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"[âŒ] Error creating vectorstore or retriever: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4a266",
   "metadata": {},
   "source": [
    "# Grade Documents -> Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d8269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'YES'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=LOCAL_LLM,\n",
    "    format=\"json\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=512,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant. It dos not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'YES' or 'NO' score to indicate whether the document is relevant to the question. \\n \n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document}\\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"how to save LLM costs?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ad91a7",
   "metadata": {},
   "source": [
    "# Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc295c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To save LLM costs, you can carefully select the right models for specific tasks, optimize agent memory, and use techniques like LLM Lingua to achieve cost savings while maintaining high performance and user experience. Additionally, staying updated with the latest research and developments in the field can help discover new cost optimization methods. This approach can reduce LLM costs by up to 78% or more.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise and to the point <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    Context: {context} \\n\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=LOCAL_LLM,\n",
    "    temperature=0.1, \n",
    "    max_tokens=512, \n",
    "    streaming=True, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# post-processing\n",
    "def format_docs(docs):\n",
    "    return (\"\\n\\n\".join(doc.page_content for doc in docs))\n",
    "\n",
    "# chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"how to save LLM costs?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\n",
    "    \"context\": docs,\n",
    "    \"question\": question,\n",
    "})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028b9ce",
   "metadata": {},
   "source": [
    "# Web Search via Tavily -> check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662180b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003872ad",
   "metadata": {},
   "source": [
    "# Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8682a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'YES'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=LOCAL_LLM,\n",
    "    format=\"json\",\n",
    "    temperature=0.1, \n",
    "    max_tokens=512, \n",
    "    streaming=True, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer is grounded in / supported by a set of facts. Give a binary score 'YES' or 'NO' score to indicate whether the answer is grounded in / supported by the facts. Provide the binary score as a JSON with a single key 'score' and no premable or explaination. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are all the facts:\n",
    "    \\n -------- \\n\n",
    "    {documents}\n",
    "    \\n -------- \\n\n",
    "    Here is the answer: {generation} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\n",
    "    \"documents\": docs,\n",
    "    \"generation\": generation,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33aaea",
   "metadata": {},
   "source": [
    "# Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad22ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'YES'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=LOCAL_LLM,\n",
    "    format=\"json\",\n",
    "    temperature=0.1, \n",
    "    max_tokens=512, \n",
    "    streaming=True, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an answer is useful to resolve a question. Give a binary score 'YES' or 'NO' score to indicate whether the answer is useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no premable or explaination. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are all the facts:\n",
    "    \\n -------- \\n\n",
    "    {generation}\n",
    "    \\n -------- \\n\n",
    "    Here is the question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\n",
    "    \"question\": question,\n",
    "    \"generation\": generation,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b30e7",
   "metadata": {},
   "source": [
    "# LangGraph -> Setup states & nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ab88bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****RETRIEVE****\n",
      "(\"Finished running: retrieve with value: {'documents': [Document(metadata={}, \"\n",
      " \"page_content='By implementing these strategies, you can reduce LLM costs by \"\n",
      " 'up to 78% or more. Remember, reducing costs while maintaining performance '\n",
      " 'and user experience is a critical skill for AI startups. Stay proactive and '\n",
      " 'continuously optimize your LLM usage to maximize efficiency and '\n",
      " 'profitability.\\\\n\\\\n\\\\u200d\\\\n\\\\nGet free HubSpot AI For Marketers Course: '\n",
      " \"[https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\\\n\\\\nðŸ”— Links'), \"\n",
      " \"Document(metadata={}, page_content='By implementing these strategies, you \"\n",
      " 'can reduce LLM costs by up to 78% or more. Remember, reducing costs while '\n",
      " 'maintaining performance and user experience is a critical skill for AI '\n",
      " 'startups. Stay proactive and continuously optimize your LLM usage to '\n",
      " 'maximize efficiency and profitability.\\\\n\\\\n\\\\u200d\\\\n\\\\nGet free HubSpot AI '\n",
      " 'For Marketers Course: '\n",
      " \"[https://clickhubspot.com/xut](https://clickhubspot.com/xut)\\\\n\\\\nðŸ”— Links'), \"\n",
      " \"Document(metadata={}, page_content='### Q: Can I reduce LLM costs without \"\n",
      " 'compromising performance?\\\\n\\\\nA: Yes, it is possible to reduce LLM costs '\n",
      " 'without compromising performance. By carefully selecting the right models '\n",
      " 'for specific tasks, optimizing agent memory, and using techniques like LLM '\n",
      " 'Lingua, you can achieve cost savings while maintaining high performance and '\n",
      " 'user experience.\\\\n\\\\n### Q: Are there any other cost optimization methods '\n",
      " 'for LLM that I should be aware of?\\\\n\\\\nA: While the methods mentioned in '\n",
      " 'this article are effective for reducing LLM costs, there may be other '\n",
      " 'innovative approaches and techniques available. Stay updated with the latest '\n",
      " 'research and developments in the field to discover new cost optimization '\n",
      " 'methods.\\\\n\\\\n## Related articles\\\\n\\\\n[Browse all '\n",
      " \"articles](https://www.ai-jason.com/)'), Document(metadata={}, \"\n",
      " \"page_content='### Q: Can I reduce LLM costs without compromising \"\n",
      " 'performance?\\\\n\\\\nA: Yes, it is possible to reduce LLM costs without '\n",
      " 'compromising performance. By carefully selecting the right models for '\n",
      " 'specific tasks, optimizing agent memory, and using techniques like LLM '\n",
      " 'Lingua, you can achieve cost savings while maintaining high performance and '\n",
      " 'user experience.\\\\n\\\\n### Q: Are there any other cost optimization methods '\n",
      " 'for LLM that I should be aware of?\\\\n\\\\nA: While the methods mentioned in '\n",
      " 'this article are effective for reducing LLM costs, there may be other '\n",
      " 'innovative approaches and techniques available. Stay updated with the latest '\n",
      " 'research and developments in the field to discover new cost optimization '\n",
      " 'methods.\\\\n\\\\n## Related articles\\\\n\\\\n[Browse all '\n",
      " \"articles](https://www.ai-jason.com/)')], 'question': 'how to save LLM \"\n",
      " \"costs?'}\")\n",
      "****CHECK DOCUMENT RELEVANCE TO QUESTION****\n",
      "****GRADE: DOCUMENT NOT RELEVANT****\n",
      "****GRADE: DOCUMENT NOT RELEVANT****\n",
      "****GRADE: DOCUMENT NOT RELEVANT****\n",
      "****GRADE: DOCUMENT NOT RELEVANT****\n",
      "****ASSESS GRADED DOCUMENTS****\n",
      "****DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH****\n",
      "(\"Finished running: grade_documents with value: {'documents': [], 'question': \"\n",
      " \"'how to save LLM costs?', 'web_search': 'YES'}\")\n",
      "****WEB SEARCH****\n",
      "(\"Finished running: websearch with value: {'documents': [Document(metadata={}, \"\n",
      " 'page_content=\"![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedaf791724343e65da125_llm%20lingua.jpg)\\\\n\\\\n## '\n",
      " '5. Optimize Agent Memory\\\\n\\\\nOptimizing agent memory is another way to '\n",
      " 'reduce LLM costs. By carefully managing the amount of conversation history '\n",
      " 'stored in memory, you can minimize the number of tokens required for each '\n",
      " 'interaction. This can lead to significant cost savings, especially when '\n",
      " 'dealing with long conversations. [...] One effective way to reduce LLM costs '\n",
      " 'is to change the model you are using. Different models have different costs '\n",
      " 'associated with them. For example, GPT-4 is the most powerful but also the '\n",
      " 'most expensive model, while Mistro 7B is significantly cheaper. By using a '\n",
      " 'smaller model for specific tasks and reserving the more expensive model for '\n",
      " 'complex questions, you can achieve significant cost '\n",
      " 'savings.\\\\n\\\\n![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedacac82767caefbdf0a0_1.jpg) '\n",
      " '[...] A: Yes, it is possible to reduce LLM costs without compromising '\n",
      " 'performance. By carefully selecting the right models for specific tasks, '\n",
      " 'optimizing agent memory, and using techniques like LLM Lingua, you can '\n",
      " 'achieve cost savings while maintaining high performance and user '\n",
      " 'experience.\\\\n\\\\n### Q: Are there any other cost optimization methods for '\n",
      " 'LLM that I should be aware of?\\\\n**3. Mix and Match Models for Hybrid '\n",
      " 'Workflows**\\\\n\\\\nAnother way to save costs is to use a combination of '\n",
      " 'models. For instance, you could use a smaller model to pre-process or filter '\n",
      " 'data and then use a larger model to generate detailed insights.\\\\n\\\\nSuppose '\n",
      " 'you are analyzing customer reviews: you might first use a smaller model to '\n",
      " 'classify reviews into positive, neutral, or negative categories, and then '\n",
      " 'only use a larger model to summarize the negative reviews or provide '\n",
      " 'actionable recommendations. [...] To further optimize expenses, consider '\n",
      " 'caching strategies that **focus on outputs:**\\\\n\\\\nBy integrating these '\n",
      " 'caching strategies, you can minimize unnecessary API calls to the LLM, '\n",
      " 'thereby reducing both token usage and latency in user-facing '\n",
      " 'applications.\\\\n\\\\n## Conclusion\\\\n\\\\nReducing the costs of using large '\n",
      " 'language models (LLMs) requires strategic approaches tailored to your '\n",
      " \"project's needs. Implementing these techniques can lead to significant \"\n",
      " 'savings. [...] Maximizing efficiency in your interactions with the model is '\n",
      " 'one of the best ways to cut LLM costs. Effective prompting is the key '\n",
      " 'hereâ€”better prompts mean fewer queries, shorter responses, and more relevant '\n",
      " 'results.\\\\n\\\\n#### Minimize Input Tokens\\\\n![Choosing between cheaper models '\n",
      " 'vs. reliable outputs '\n",
      " 'meme](/_next/image?url=%2Fstatic%2Fblog%2Fmonitor-and-optimize-llm-costs%2Fmeme-1.webp&w=1920&q=75)\\\\n\\\\n![Choosing '\n",
      " 'between cheaper models vs. reliable outputs '\n",
      " 'meme](/_next/image?url=%2Fstatic%2Fblog%2Fmonitor-and-optimize-llm-costs%2Fmeme-1.webp&w=1920&q=75)\\\\n\\\\n## '\n",
      " '1. Optimize Prompt Engineering\\\\n\\\\nOptimizing your prompts is one of the '\n",
      " 'simplest yet most effective ways to reduce LLM costs. Inefficient prompts '\n",
      " 'waste tokens and drive up costs. [...] Most developers see a 30-50% '\n",
      " 'reduction in LLM costs by implementing prompt optimization and caching '\n",
      " 'alone. Comprehensive implementation of all five strategies can reduce costs '\n",
      " 'by up to 90% in specific use cases.\\\\n\\\\n### Which optimization technique '\n",
      " 'provides the fastest results? [...] Remember, the key is to find the right '\n",
      " 'balance between cost-efficiency and performance that works best for your '\n",
      " 'specific use case. By implementing these techniques and utilizing '\n",
      " 'observability platforms, you can reduce your LLM costs by up to 90% without '\n",
      " 'compromising on quality.\\\\n\\\\n### You might be interested in:\\\\n\\\\nHow to '\n",
      " 'Reduce LLM Hallucination in Production Apps\\\\n\\\\nBest Prompt Engineering '\n",
      " 'Tools & Techniques [Updated Jan 2025]\\\\n\\\\n4 Helicone Features to Optimize '\n",
      " \"your AI App's Performance\\\\ncost-saving strategies for LLMs! optimize \"\n",
      " 'prompts, use smaller models, cache responses, batch requests, compress '\n",
      " 'prompts, quantize models,\\\\n## How RAG Works:\\\\n\\\\nBy incorporating RAG into '\n",
      " 'your LLM architecture, you can significantly reduce the number of tokens '\n",
      " 'processed per request, leading to lower computational costs and improved '\n",
      " 'efficiency.\\\\n\\\\n# Cost Optimization Strategies\\\\n\\\\nOptimizing the cost of '\n",
      " 'LLMs requires a multi-pronged approach, targeting various aspects of the '\n",
      " 'modelâ€™s lifecycle. Below are detailed strategies to help you achieve '\n",
      " 'significant cost savings.\\\\n\\\\n## 1. Chunking: Logical and Context-Aware '\n",
      " 'Data Processing [...] # Retrieval-Augmented Generation (RAG): Enhancing '\n",
      " 'Efficiency\\\\n\\\\nRetrieval-Augmented Generation (RAG) is a cutting-edge '\n",
      " 'architectural approach that can significantly reduce the operational costs '\n",
      " 'of LLMs. RAG integrates real-time data retrieval with LLMs, allowing the '\n",
      " 'model to access external data sources when generating responses. This '\n",
      " 'approach not only enhances the modelâ€™s accuracy and relevance but also '\n",
      " 'optimizes resource usage by reducing the reliance on purely generative '\n",
      " 'processes. [...] Frequently asked questions, greetings, and other repetitive '\n",
      " 'interactions can unnecessarily burden LLMs, driving up costs. Semantic '\n",
      " 'caching mechanisms can alleviate this by storing and retrieving common '\n",
      " 'responses.\\\\n\\\\n**Tools and Techniques:**\\\\n\\\\n## 3. Search Space '\n",
      " 'Optimization: Focusing on Relevance\")], \\'question\\': \\'how to save LLM '\n",
      " \"costs?'}\")\n",
      "****GENERATE****\n",
      "****CHECK HALLUCINATIONS****\n",
      "****DECISION: GENERATION IS GROUNDED IN DOCUMENTS****\n",
      "****GRADE GENERATION vs QUESTION****\n",
      "****DECISION: GENERATION ADDRESSES QUESTION****\n",
      "(\"Finished running: generate with value: {'documents': [Document(metadata={}, \"\n",
      " 'page_content=\"![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedaf791724343e65da125_llm%20lingua.jpg)\\\\n\\\\n## '\n",
      " '5. Optimize Agent Memory\\\\n\\\\nOptimizing agent memory is another way to '\n",
      " 'reduce LLM costs. By carefully managing the amount of conversation history '\n",
      " 'stored in memory, you can minimize the number of tokens required for each '\n",
      " 'interaction. This can lead to significant cost savings, especially when '\n",
      " 'dealing with long conversations. [...] One effective way to reduce LLM costs '\n",
      " 'is to change the model you are using. Different models have different costs '\n",
      " 'associated with them. For example, GPT-4 is the most powerful but also the '\n",
      " 'most expensive model, while Mistro 7B is significantly cheaper. By using a '\n",
      " 'smaller model for specific tasks and reserving the more expensive model for '\n",
      " 'complex questions, you can achieve significant cost '\n",
      " 'savings.\\\\n\\\\n![](https://cdn.prod.website-files.com/647a9c825f099afe643b5c78/65bedacac82767caefbdf0a0_1.jpg) '\n",
      " '[...] A: Yes, it is possible to reduce LLM costs without compromising '\n",
      " 'performance. By carefully selecting the right models for specific tasks, '\n",
      " 'optimizing agent memory, and using techniques like LLM Lingua, you can '\n",
      " 'achieve cost savings while maintaining high performance and user '\n",
      " 'experience.\\\\n\\\\n### Q: Are there any other cost optimization methods for '\n",
      " 'LLM that I should be aware of?\\\\n**3. Mix and Match Models for Hybrid '\n",
      " 'Workflows**\\\\n\\\\nAnother way to save costs is to use a combination of '\n",
      " 'models. For instance, you could use a smaller model to pre-process or filter '\n",
      " 'data and then use a larger model to generate detailed insights.\\\\n\\\\nSuppose '\n",
      " 'you are analyzing customer reviews: you might first use a smaller model to '\n",
      " 'classify reviews into positive, neutral, or negative categories, and then '\n",
      " 'only use a larger model to summarize the negative reviews or provide '\n",
      " 'actionable recommendations. [...] To further optimize expenses, consider '\n",
      " 'caching strategies that **focus on outputs:**\\\\n\\\\nBy integrating these '\n",
      " 'caching strategies, you can minimize unnecessary API calls to the LLM, '\n",
      " 'thereby reducing both token usage and latency in user-facing '\n",
      " 'applications.\\\\n\\\\n## Conclusion\\\\n\\\\nReducing the costs of using large '\n",
      " 'language models (LLMs) requires strategic approaches tailored to your '\n",
      " \"project's needs. Implementing these techniques can lead to significant \"\n",
      " 'savings. [...] Maximizing efficiency in your interactions with the model is '\n",
      " 'one of the best ways to cut LLM costs. Effective prompting is the key '\n",
      " 'hereâ€”better prompts mean fewer queries, shorter responses, and more relevant '\n",
      " 'results.\\\\n\\\\n#### Minimize Input Tokens\\\\n![Choosing between cheaper models '\n",
      " 'vs. reliable outputs '\n",
      " 'meme](/_next/image?url=%2Fstatic%2Fblog%2Fmonitor-and-optimize-llm-costs%2Fmeme-1.webp&w=1920&q=75)\\\\n\\\\n![Choosing '\n",
      " 'between cheaper models vs. reliable outputs '\n",
      " 'meme](/_next/image?url=%2Fstatic%2Fblog%2Fmonitor-and-optimize-llm-costs%2Fmeme-1.webp&w=1920&q=75)\\\\n\\\\n## '\n",
      " '1. Optimize Prompt Engineering\\\\n\\\\nOptimizing your prompts is one of the '\n",
      " 'simplest yet most effective ways to reduce LLM costs. Inefficient prompts '\n",
      " 'waste tokens and drive up costs. [...] Most developers see a 30-50% '\n",
      " 'reduction in LLM costs by implementing prompt optimization and caching '\n",
      " 'alone. Comprehensive implementation of all five strategies can reduce costs '\n",
      " 'by up to 90% in specific use cases.\\\\n\\\\n### Which optimization technique '\n",
      " 'provides the fastest results? [...] Remember, the key is to find the right '\n",
      " 'balance between cost-efficiency and performance that works best for your '\n",
      " 'specific use case. By implementing these techniques and utilizing '\n",
      " 'observability platforms, you can reduce your LLM costs by up to 90% without '\n",
      " 'compromising on quality.\\\\n\\\\n### You might be interested in:\\\\n\\\\nHow to '\n",
      " 'Reduce LLM Hallucination in Production Apps\\\\n\\\\nBest Prompt Engineering '\n",
      " 'Tools & Techniques [Updated Jan 2025]\\\\n\\\\n4 Helicone Features to Optimize '\n",
      " \"your AI App's Performance\\\\ncost-saving strategies for LLMs! optimize \"\n",
      " 'prompts, use smaller models, cache responses, batch requests, compress '\n",
      " 'prompts, quantize models,\\\\n## How RAG Works:\\\\n\\\\nBy incorporating RAG into '\n",
      " 'your LLM architecture, you can significantly reduce the number of tokens '\n",
      " 'processed per request, leading to lower computational costs and improved '\n",
      " 'efficiency.\\\\n\\\\n# Cost Optimization Strategies\\\\n\\\\nOptimizing the cost of '\n",
      " 'LLMs requires a multi-pronged approach, targeting various aspects of the '\n",
      " 'modelâ€™s lifecycle. Below are detailed strategies to help you achieve '\n",
      " 'significant cost savings.\\\\n\\\\n## 1. Chunking: Logical and Context-Aware '\n",
      " 'Data Processing [...] # Retrieval-Augmented Generation (RAG): Enhancing '\n",
      " 'Efficiency\\\\n\\\\nRetrieval-Augmented Generation (RAG) is a cutting-edge '\n",
      " 'architectural approach that can significantly reduce the operational costs '\n",
      " 'of LLMs. RAG integrates real-time data retrieval with LLMs, allowing the '\n",
      " 'model to access external data sources when generating responses. This '\n",
      " 'approach not only enhances the modelâ€™s accuracy and relevance but also '\n",
      " 'optimizes resource usage by reducing the reliance on purely generative '\n",
      " 'processes. [...] Frequently asked questions, greetings, and other repetitive '\n",
      " 'interactions can unnecessarily burden LLMs, driving up costs. Semantic '\n",
      " 'caching mechanisms can alleviate this by storing and retrieving common '\n",
      " 'responses.\\\\n\\\\n**Tools and Techniques:**\\\\n\\\\n## 3. Search Space '\n",
      " 'Optimization: Focusing on Relevance\")], \\'question\\': \\'how to save LLM '\n",
      " \"costs?', 'generation': 'To save LLM costs, consider optimizing agent memory, \"\n",
      " 'using a combination of models (hybrid workflows), caching strategies that '\n",
      " 'focus on outputs, minimizing input tokens, and optimizing prompt '\n",
      " 'engineering. These techniques can lead to significant cost savings, with '\n",
      " 'comprehensive implementation reducing costs by up to 90%. Effective '\n",
      " 'prompting is key to minimizing token usage and latency in user-facing '\n",
      " \"applications.'}\")\n",
      "To save LLM costs, consider optimizing agent memory, using a combination of models (hybrid workflows), caching strategies that focus on outputs, minimizing input tokens, and optimizing prompt engineering. These techniques can lead to significant cost savings, with comprehensive implementation reducing costs by up to 90%. Effective prompting is key to minimizing token usage and latency in user-facing applications.\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "# define graph state\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "    \n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "# define graph flow\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve docs from vectorstore\n",
    "    \n",
    "    Args:\n",
    "        state (dict): the current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): new key added to the state, docs, that contains the retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"****RETRIEVE****\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "    }\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved docs are relevant to the question.\n",
    "    If any doc is relevant, we'll set a flag to run web search (for the same).\n",
    "\n",
    "    Args:\n",
    "        state (dict): the current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): filtered out irrelevant docs, and updated web_search state\n",
    "    \"\"\"\n",
    "    print(\"****CHECK DOCUMENT RELEVANCE TO QUESTION****\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"NO\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\n",
    "            \"question\": question,\n",
    "            \"document\": d.page_content,\n",
    "        })\n",
    "        grade = score['score']\n",
    "        \n",
    "        # doc relevant\n",
    "        if grade.lower() == \"YES\":\n",
    "            print(\"****GRADE: DOCUMENT RELEVANT****\")\n",
    "            filtered_docs.append(d)\n",
    "        \n",
    "        # doc not relevant\n",
    "        else:\n",
    "            print(\"****GRADE: DOCUMENT NOT RELEVANT****\")\n",
    "            # we do not include the doc in filtered_docs\n",
    "            # we set a flag to indicate that we want to run web search\n",
    "            web_search = \"YES\"\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"web_search\": web_search,\n",
    "    }\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved docs\n",
    "    \n",
    "    Args:\n",
    "        state (dict): the current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): new key added to the state, generation, that contains the LLM generation\n",
    "    \"\"\"\n",
    "    print(\"****GENERATE****\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # rag gen\n",
    "    generation = rag_chain.invoke({\n",
    "        \"context\": documents,\n",
    "        \"question\": question,\n",
    "    })\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation,\n",
    "    }\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "    \n",
    "    Args:\n",
    "        state (dict): the current graph state\n",
    "    \n",
    "    Returns:\n",
    "        state (dict): appended web results to doc\n",
    "    \"\"\"\n",
    "    print(\"****WEB SEARCH****\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = (\"\\n\".join([d[\"content\"] for d in docs]))\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    \n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "    }\n",
    "\n",
    "# conditional edges\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "    \n",
    "    Args:\n",
    "        state (dict): the current graph state\n",
    "    \n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"****ASSESS GRADED DOCUMENTS****\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_docs = state[\"documents\"]\n",
    "    \n",
    "    if web_search == \"YES\":\n",
    "        # all docs have been filtered check_relevance\n",
    "        # we'll regenerate a new query\n",
    "        print(\"****DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH****\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # we have relevant docs, we can generate an answer\n",
    "        print(\"****DECISION: ALL DOCUMENTS ARE RELEVANT TO QUESTION, GENERATE ANSWER****\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"****CHECK HALLUCINATIONS****\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score['score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"YES\":\n",
    "        print(\"****DECISION: GENERATION IS GROUNDED IN DOCUMENTS****\")\n",
    "        # Check question-answering\n",
    "        print(\"****GRADE GENERATION vs QUESTION****\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"YES\":\n",
    "            print(\"****DECISION: GENERATION ADDRESSES QUESTION****\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"****DECISION: GENERATION DOES NOT ADDRESS QUESTION****\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"****DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY****\")\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "# compile graph\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# define nodes\n",
    "workflow.add_node(\"websearch\", web_search)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# compile graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# test graph\n",
    "from pprint import pprint\n",
    "\n",
    "inputs = {\"question\": \"how to save LLM costs?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key} with value: {value}\")\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee471478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
